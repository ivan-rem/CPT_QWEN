# Paths & data
model_name: "Qwen/Qwen2.5-0.5B"
output_dir: "out/qwen2.5-0.5b-cpt"
logging_dir: "out/qwen2.5-0.5b-cpt/logs"
train_files: "src\\data\\cpt_data.parquet"   # parquet files with a 'text' column
text_column: "text"

# Sequence / packing
max_seq_len: 1024
pack_sequences: true
shuffle_buffer_size: 50_000  # for streaming shuffling

# Training hyperparams (rough defaults for ~1B tokens)
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 32
learning_rate: 5.0e-5
warmup_ratio: 0.03
weight_decay: 0.1
lr_scheduler_type: "cosine"
max_steps: 1000
save_steps: 200
eval_steps: 200
logging_steps: 10
max_grad_norm: 1.0

# Precision & performance
bf16: false
fp16: true
gradient_checkpointing: true
flash_attention_2: false

# Distributed / deepspeed
use_deepspeed: false              # set true + provide deepspeed_config if needed
zero_stage: 2                     # if deepspeed

# WandB / logging
report_to: ["wandb"]
wandb_project: "qwen-local-test"
wandb_run_name: "qwen2.5-0.5b-local-test"

# Misc
save_total_limit: 3
seed: 42
resume_from_checkpoint: true